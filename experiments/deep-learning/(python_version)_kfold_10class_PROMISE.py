# -*- coding: utf-8 -*-
"""CibSE_kfold_&_traintest_10class_[dataset_PROMISE].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KSNmYnMkh1mk9saHrV_h0vn1GnS83LIa?usp=sharing

### ***Install Dependencies***
"""

# !pip install -q accelerate Evaluate
# !pip list

"""## ***Import dependencies***"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
import time
import os
import torch
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    balanced_accuracy_score,
    accuracy_score, f1_score, precision_score, recall_score,
    precision_recall_fscore_support,
    confusion_matrix, classification_report,
)
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    set_seed,
    AutoConfig
)
from datasets import Dataset
os.environ["WANDB_DISABLED"] = "true"

"""## ***Seed***"""

# Fijar semilla global
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

"""# ***TRAIN K-FOLD PIPELINE***

## ***Utils***
"""

def plot_confusion_matrix(y_true, y_pred, id2label, title="Matriz de Confusión"):
    """
    Función genérica para graficar matriz de confusión
    y_true: etiquetas reales
    y_pred: etiquetas predichas
    id2label: diccionario {id: clase}
    """
    cm = confusion_matrix(y_true, y_pred)
    cm_df = pd.DataFrame(
        cm,
        index=[id2label[i] for i in range(len(id2label))],
        columns=[id2label[i] for i in range(len(id2label))]
    )

    print("\nMatriz de Confusión (Frecuencias):")
    print(cm_df)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')
    plt.ylabel('Clase real')
    plt.xlabel('Predicción')
    plt.title(title)
    plt.show()

def plot_training_curves(trainer):
    """
    Grafica únicamente train loss vs validation loss
    en base a los logs de HuggingFace Trainer.
    """
    logs = pd.DataFrame(trainer.state.log_history)

    # Extraer valores de pérdida
    train_loss = logs[['epoch', 'loss']].dropna()
    val_loss   = logs[['epoch', 'eval_loss']].dropna()

    plt.figure(figsize=(8,6))
    plt.plot(train_loss['epoch'], train_loss['loss'], marker='o', label="Train Loss")
    plt.plot(val_loss['epoch'], val_loss['eval_loss'], marker='o', label="Validation Loss")
    plt.xlabel("Época")
    plt.ylabel("Loss")
    plt.title("Curva de entrenamiento (Loss)")
    plt.legend()
    plt.grid(True)
    plt.show()

def plot_kfold_final_confusion_matrix(conf_matrix_total, id2label):
    """
    Grafica matriz de confusión final (10-Fold)
    """
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        conf_matrix_total,
        annot=True,
        fmt="d",
        cmap="Blues",
        xticklabels=list(id2label.values()),
        yticklabels=list(id2label.values())
    )
    plt.xlabel("Predicción")
    plt.ylabel("Real")
    plt.title("Matriz de Confusión Final (10-Fold)")
    plt.show()

"""## ***Pipeline: run kfold function***"""

def run_kfold_experiment(df, model_name, max_length=512, lr=5e-5, batch_size=8, n_splits=10):
    """
    Engloba el entrenamiento con K-Fold Cross Validation parametrizado y logs de tiempo.
    """
    # ==================== 1. Preparación de etiquetas ==============================
    classes_to_keep = ['O', 'PE', 'SE', 'US', 'LF', 'A', 'FT', 'SC', 'L', 'MN']
    df = df[df['class'].isin(classes_to_keep)].copy()

    print("\n----- Class Distribution: -----\n")
    for cls, count in df['class'].value_counts().items():
        print(f"  {cls:<20} : {count:>6}")
    print("-" * 25)

    # Mapear clases a IDs
    label2id = {label: idx for idx, label in enumerate(df['class'].unique())}
    id2label = {idx: label for label, idx in label2id.items()}
    df['label'] = df['class'].map(label2id)

    X = df['RequirementText'].values
    y = df['label'].values

    # ==================== 2. Tokenizer ====================
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    def tokenize_function(examples):
        return tokenizer(
            examples["RequirementText"],
            padding="max_length",
            truncation=True,
            max_length=max_length
        )

    # ==================== 3. Métricas de evaluación ====================
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return {
            "accuracy": accuracy_score(labels, predictions),
            "f1": f1_score(labels, predictions, average='weighted', zero_division=0),
            "precision": precision_score(labels, predictions, average='weighted', zero_division=0),
            "recall": recall_score(labels, predictions, average='weighted', zero_division=0),
        }

    # ==================== 4. K-Fold Setup y Acumuladores ====================
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)

    all_metrics = {"acc": [], "f1_m": [], "prec_m": [], "rec_m": [], "f1_w": [], "prec_w": [], "rec_w": []}
    per_class_metrics = {cls: {"precision": [], "recall": [], "f1": []} for cls in label2id.keys()}
    conf_matrix_total = np.zeros((len(label2id), len(label2id)), dtype=int)

    # Listas para logs de tiempo
    total_start_time = time.time()
    fold_times = [] # tiempo/fold
    epochs_trained = [] # epocas/fold
    epoch_durations = [] # tiempo promedio por epoca/fold

    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        print(f"\n==================== FOLD {fold+1}/{n_splits} ====================")
        fold_start_time = time.time()

        train_dataset = Dataset.from_pandas(df.iloc[train_idx])
        val_dataset = Dataset.from_pandas(df.iloc[val_idx])

        tokenized_train = train_dataset.map(tokenize_function, batched=True)
        tokenized_val = val_dataset.map(tokenize_function, batched=True)

        model = AutoModelForSequenceClassification.from_pretrained(
            model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id
        )

        # Congelar capas iniciales (fine-tunning)
        layers = model.base_model.encoder.layer
        for layer in layers[:6]: # Congela 6 de 12
            for param in layer.parameters():
                param.requires_grad = False

        training_args = TrainingArguments(
            output_dir=f"./results_10class_length{max_length}/fold_{fold+1}",
            eval_strategy="epoch",
            save_strategy="epoch",
            logging_strategy="epoch",
            learning_rate=lr,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            num_train_epochs=10,
            weight_decay=0.01,
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            logging_dir=f"./logs/results_10class_length{max_length}/fold_{fold+1}",
            report_to="none", # tensorboard
            save_total_limit=1
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_train,
            eval_dataset=tokenized_val,
            compute_metrics=compute_metrics,
            # data_collator=data_collator
            # tokenizer=tokenizer,
            # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
        )

        # Entrenamiento
        train_result = trainer.train()

        # --- [LOGS: Tiempos] ---
        fold_end_time = time.time()
        fold_duration = fold_end_time - fold_start_time
        actual_epochs = trainer.state.epoch
        train_runtime = train_result.metrics["train_runtime"]
        fold_times.append(fold_duration)
        epochs_trained.append(actual_epochs)
        epoch_durations.append(train_runtime / actual_epochs)

        # --- Procesamiento de Resultados ---
        preds = trainer.predict(tokenized_val)
        pred_labels = np.argmax(preds.predictions, axis=1)
        true_labels = df.iloc[val_idx]['label'].values

        all_metrics["acc"].append(accuracy_score(true_labels, pred_labels))
        all_metrics["f1_m"].append(f1_score(true_labels, pred_labels, average='macro', zero_division=0))
        all_metrics["prec_m"].append(precision_score(true_labels, pred_labels, average='macro', zero_division=0))
        all_metrics["rec_m"].append(recall_score(true_labels, pred_labels, average='macro', zero_division=0))
        all_metrics["f1_w"].append(f1_score(true_labels, pred_labels, average='weighted', zero_division=0))
        all_metrics["prec_w"].append(precision_score(true_labels, pred_labels, average='weighted', zero_division=0))
        all_metrics["rec_w"].append(recall_score(true_labels, pred_labels, average='weighted', zero_division=0))

        # Acumular Matriz de confusion
        conf_matrix_total += confusion_matrix(true_labels, pred_labels, labels=list(range(len(label2id))))

        # Métricas por clase de este fold
        precs, recs, f1s, _ = precision_recall_fscore_support(
            true_labels, pred_labels, labels=list(range(len(label2id))), zero_division=0
        )
        for i, cls in id2label.items():
            per_class_metrics[cls]["precision"].append(precs[i])
            per_class_metrics[cls]["recall"].append(recs[i])
            per_class_metrics[cls]["f1"].append(f1s[i])

    # ==================== 5. Resultados ====================
    total_end_time = time.time()
    total_duration_min = (total_end_time - total_start_time) / 60

    print("\n" + "="*40)
    print("REPORTE DE TIEMPOS DE EJECUCIÓN")
    print("="*40)
    print(f"Tiempo/época promedio (s)   : {np.mean(epoch_durations):.2f} s")
    print(f"Tiempo/fold promedio (min)  : {(np.mean(fold_times)/60):.2f} min")
    print(f"Tiempo Total (min)          : {total_duration_min:.2f} min")
    print(f"Nro Épocas/fold promedio    : {np.mean(epochs_trained):.1f} épocas")

    print("\n" + "="*40)
    print(f"RESULTADOS FINALES (MAX_LENGTH: {max_length}, BATCH_SIZE: {batch_size}, LR: {lr})")
    print("="*40)
    print(f"Accuracy promedio: {np.mean(all_metrics['acc']):.4f}")

    print(f"Precision Weighted promedio: {np.mean(all_metrics['prec_w']):.4f}")
    print(f"Recall Weighted promedio: {np.mean(all_metrics['rec_w']):.4f}")
    print(f"F1-Score Weighted promedio: {np.mean(all_metrics['f1_w']):.4f}")

    print(f"Precision Macro promedio: {np.mean(all_metrics['prec_m']):.4f}")
    print(f"Recall Macro promedio: {np.mean(all_metrics['rec_m']):.4f}")
    print(f"F1-Score Macro promedio: {np.mean(all_metrics['f1_m']):.4f}")

    print("\n===== Matriz de Confusión Acumulada =====")
    class_names = [id2label[i] for i in range(len(id2label))]
    cm_df = pd.DataFrame(
        conf_matrix_total,
        index=class_names,
        columns=class_names
    )
    cm_df.index.name = 'True\Pred' # Asignamos el indicador en la esquina
    # Forzamos a Pandas a mostrar todas las columnas sin saltos de línea
    with pd.option_context('display.max_columns', None, 'display.width', 1000):
        print(cm_df)

    print("\n--- Métricas por Clase (Promedio de 10 Pliegues) ---")
    for cls in per_class_metrics.keys():
        prec_mean, prec_std = np.mean(per_class_metrics[cls]["precision"]), np.std(per_class_metrics[cls]["precision"])
        rec_mean, rec_std   = np.mean(per_class_metrics[cls]["recall"]), np.std(per_class_metrics[cls]["recall"])
        f1_mean, f1_std     = np.mean(per_class_metrics[cls]["f1"]), np.std(per_class_metrics[cls]["f1"])

        print(f"Clase: {cls}")
        print(f"  - Precision: {prec_mean:.4f} (+/- {prec_std:.4f})")
        print(f"  - Recall:    {rec_mean:.4f} (+/- {rec_std:.4f})")
        print(f"  - F1-Score:  {f1_mean:.4f} (+/- {f1_std:.4f})")

"""## ***Experiments English***

#### ***Prueba 1 (MAX_LENGTH=32, BATCH_SIZE=8)***
"""

print("\n--- INICIANDO EXPERIMENTO 1 (MAX_LENGTH=32, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr.csv')
model_name="microsoft/mpnet-base"
run_kfold_experiment(df_original, model_name, max_length=32, lr=5e-5, batch_size=8, n_splits=10)

"""#### ***Prueba 2 (MAX_LENGTH=64, BATCH_SIZE=8)***"""

print("\n--- INICIANDO EXPERIMENTO 2 (MAX_LENGTH=64, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr.csv')
model_name="microsoft/mpnet-base"
run_kfold_experiment(df_original, model_name, max_length=64, lr=5e-5, batch_size=8, n_splits=10)

"""#### ***Prueba 3 (MAX_LENGTH=128, BATCH_SIZE=8)***"""

print("\n--- INICIANDO EXPERIMENTO 3 (MAX_LENGTH=128, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr.csv')
model_name="microsoft/mpnet-base"
run_kfold_experiment(df_original, model_name, max_length=128, lr=5e-5, batch_size=8, n_splits=10)

"""#### ***Prueba 4 (MAX_LENGTH=512, BATCH_SIZE=8)***"""

print("\n--- INICIANDO EXPERIMENTO 4 (MAX_LENGTH=512, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr.csv')
model_name="microsoft/mpnet-base"
run_kfold_experiment(df_original, model_name, max_length=512, lr=5e-5, batch_size=8, n_splits=10)

"""## ***Experiments Portugues***

#### ***Prueba 1 (MAX_LENGTH=32, BATCH_SIZE=8)***
"""

print("\n--- INICIANDO EXPERIMENTO 1 (MAX_LENGTH=32, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr_portuguese.csv')
model_name = "neuralmind/bert-base-portuguese-cased"
run_kfold_experiment(df_original, model_name, max_length=32, lr=5e-5, batch_size=8, n_splits=10)

"""#### ***Prueba 2 (MAX_LENGTH=64, BATCH_SIZE=8)***"""

print("\n--- INICIANDO EXPERIMENTO 2 (MAX_LENGTH=64, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr_portuguese.csv')
model_name = "neuralmind/bert-base-portuguese-cased"
run_kfold_experiment(df_original, model_name, max_length=64, lr=5e-5, batch_size=8, n_splits=10)

"""#### ***Prueba 3 (MAX_LENGTH=128, BATCH_SIZE=8)***"""

print("\n--- INICIANDO EXPERIMENTO 3 (MAX_LENGTH=128, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr_portuguese.csv')
model_name = "neuralmind/bert-base-portuguese-cased"
run_kfold_experiment(df_original, model_name, max_length=128, lr=5e-5, batch_size=8, n_splits=10)

"""#### ***Prueba 4 (MAX_LENGTH=512, BATCH_SIZE=8)***"""

print("\n--- INICIANDO EXPERIMENTO 4 (MAX_LENGTH=512, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr_portuguese.csv')
model_name = "neuralmind/bert-base-portuguese-cased"
run_kfold_experiment(df_original, model_name, max_length=512, lr=5e-5, batch_size=8, n_splits=10)

"""## ***Experiments Spanish***

#### ***Prueba 1 (MAX_LENGTH=32, BATCH_SIZE=8)***
"""

print("\n--- INICIANDO EXPERIMENTO 1 (MAX_LENGTH=32, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr_spanish.csv')
model_name = "dccuchile/bert-base-spanish-wwm-cased"
run_kfold_experiment(df_original, model_name, max_length=32, lr=5e-5, batch_size=8, n_splits=10)

"""#### ***Prueba 2 (MAX_LENGTH=64, BATCH_SIZE=8)***"""

print("\n--- INICIANDO EXPERIMENTO 2 (MAX_LENGTH=64, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr_spanish.csv')
model_name = "dccuchile/bert-base-spanish-wwm-cased"
run_kfold_experiment(df_original, model_name, max_length=64, lr=5e-5, batch_size=8, n_splits=10)

"""#### ***Prueba 3 (MAX_LENGTH=128, BATCH_SIZE=8)***"""

print("\n--- INICIANDO EXPERIMENTO 2 (MAX_LENGTH=128, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr_spanish.csv')
model_name = "dccuchile/bert-base-spanish-wwm-cased"
run_kfold_experiment(df_original, model_name, max_length=128, lr=5e-5, batch_size=8, n_splits=10)

"""#### ***Prueba 4 (MAX_LENGTH=512, BATCH_SIZE=8)***"""

print("\n--- INICIANDO EXPERIMENTO 2 (MAX_LENGTH=64, BATCH_SIZE=8) ---")
df_original = pd.read_csv('promise_nfr_spanish.csv')
model_name = "dccuchile/bert-base-spanish-wwm-cased"
run_kfold_experiment(df_original, model_name, max_length=512, lr=5e-5, batch_size=8, n_splits=10)

# !rm -rf results_multiclass/
# !rm -rf results4class
# !rm -rf results10class
# !rm -rf results12class